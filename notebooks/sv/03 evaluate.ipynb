{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import regex\n",
    "import unicodecsv as csv\n",
    "import lemmy\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMS_FILE = \"./data/norms.csv\"\n",
    "UD_TRAIN_FILE = \"./data/UD_Swedish-Talbanken/sv_talbanken-ud-train.conllu\"\n",
    "UD_DEV_FILE = \"./data/UD_Swedish-Talbanken/sv_talbanken-ud-dev.conllu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the normalization rules we build in the first notebook. When evaluating, we apply these to the lemmas specified in UD. Otherwise we would risk, for example, counting \"akvarie\" as the incorrect lemma for \"akvarier\" if UD specified the other spelling, \"akvarium\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NORMS_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3b55e38583c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m norm_lookup = dict([row for row in csv.reader(open(NORMS_FILE, 'rb'), delimiter=\",\",\n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_MINIMAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         lineterminator='\\n')][1:])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NORMS_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "norm_lookup = dict([row for row in csv.reader(open(NORMS_FILE, 'rb'), delimiter=\",\",\n",
    "                        quotechar='\"',\n",
    "                        quoting=csv.QUOTE_MINIMAL,\n",
    "                        encoding='utf-8',\n",
    "                        lineterminator='\\n')][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a few more normalization rules. This is due to DSN and UD not agreeing on the lemmas for certain words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_dsn_normalization = (('PRON', 'det', 'det', 'den'),\n",
    "                        ('ADJ', 'flere', 'mange', 'flere'),\n",
    "                        ('ADJ', 'mere', 'meget', 'mere'),\n",
    "                        ('ADJ', 'meget', 'meget', 'megen'),\n",
    "                        ('ADJ', 'fleste', 'mange', 'flest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = lemmy.load('sv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_ud_line(line):\n",
    "    return line.split(\"\\t\")[1:4]\n",
    "\n",
    "def _evaluate(ud_file):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    ambiguous = 0\n",
    "    mistakes = {}\n",
    "    ambiguities = {}\n",
    "    pos_prev = \"\"\n",
    "    for line in open(ud_file).readlines():\n",
    "        if line.startswith(\"#\") or line.strip() == \"\":\n",
    "            pos_prev = \"\"\n",
    "            continue\n",
    "\n",
    "        orth, lemma_expected, pos = _parse_ud_line(line)\n",
    "\n",
    "        #if pos == \"NOUN\" and lemma_expected in norm_lookup:\n",
    "        #    lemma_expected = norm_lookup[lemma_expected]\n",
    "        #else:\n",
    "        #    for pos_, orth_, expected_ud, expected_dsn in ud_dsn_normalization:\n",
    "        #        if pos != pos_ or orth.lower() != orth_ or lemma_expected != expected_ud:\n",
    "        #            continue            \n",
    "        #        lemma_expected = expected_dsn\n",
    "\n",
    "        lemmas_actual = lemmatizer.lemmatize(pos, orth.lower(), pos_previous=pos_prev)    \n",
    "        lemma_actual = lemmas_actual[0]\n",
    "\n",
    "        if len(lemmas_actual) > 1:\n",
    "            ambiguous += 1\n",
    "            ambiguities[(pos, orth)] = ambiguities.get((pos, orth), 0) + 1\n",
    "        elif lemma_actual.lower() == lemma_expected.lower():\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes[(pos, orth, lemma_expected, lemma_actual)] = mistakes.get((pos, orth, lemma_expected, lemma_actual), 0) + 1\n",
    "            incorrect += 1\n",
    "        pos_prev = pos\n",
    "\n",
    "    print(\"* correct:\", correct)\n",
    "    print(\"* incorrect:\", incorrect)\n",
    "    print(\"* ambiguous:\", ambiguous)\n",
    "    print(\"*\", correct/(incorrect+ambiguous+correct))\n",
    "    print(\"*\", (correct+ambiguous)/(incorrect+ambiguous+correct))\n",
    "    \n",
    "    return mistakes, ambiguities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on UD Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* correct: 65986\n",
      "* incorrect: 565\n",
      "* ambiguous: 122\n",
      "* 0.9896959788820062\n",
      "* 0.9915258050485204\n"
     ]
    }
   ],
   "source": [
    "mistakes_train, ambiguities_train = _evaluate(UD_TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on UD Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* correct: 9590\n",
      "* incorrect: 190\n",
      "* ambiguous: 19\n",
      "* 0.9786712929890805\n",
      "* 0.9806102663537095\n"
     ]
    }
   ],
   "source": [
    "mistakes_dev, ambiguities_dev =_evaluate(UD_DEV_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ADJ', 'andra', 'annan', 'andra'), 108),\n",
       " (('ADJ', 'första', 'en', 'första'), 51),\n",
       " (('ADV', 'tidigare', 'tidig', 'tidigare'), 40),\n",
       " (('ADJ', 'andra', 'två', 'andra'), 17),\n",
       " (('ADV', 'högst', 'hög', 'högst'), 15),\n",
       " (('ADV', 'senare', 'sen', 'senare'), 15),\n",
       " (('PRON', 'den', 'en', 'den'), 12),\n",
       " (('PRON', 'en', 'en', 'man'), 8),\n",
       " (('ADJ', 'Europeiska', 'Europeiska', 'europeisk'), 8),\n",
       " (('ADJ', 'fjärde', 'fyra', 'fjärde'), 8)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(mistakes_train.items(), key=lambda x: (-x[1], x[0][1].lower(), x))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ADV', 'mindre'), 15),\n",
       " (('ADJ', 'flera'), 12),\n",
       " (('NOUN', 'mån'), 10),\n",
       " (('DET', 'var'), 10),\n",
       " (('ADV', 'längre'), 9),\n",
       " (('ADJ', 'övrigt'), 8),\n",
       " (('NOUN', 'år'), 7),\n",
       " (('ADV', 'minst'), 6),\n",
       " (('PRON', 'vars'), 6),\n",
       " (('VERB', 'sköts'), 5)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(ambiguities_train.items(), key=lambda x: (-x[1], x[0][1].lower(), x))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vara']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"AUX\", \"är\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.rules['AUX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
